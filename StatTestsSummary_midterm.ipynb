{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Confidence Intervals & Hypothesis Testing ####\n- Confidence interval is a form of interval estimation of a population parameter, e.g. $\\mu$ i.e. the interval $\\hat{\\mu}_{L} < \\mu < \\hat{\\mu}_{U}$ in $Pr(\\hat{\\mu}_{L} < \\mu < \\hat{\\mu}_{U}) = 1-\\alpha$ is a $100(1-\\alpha)\\%$ confidence interval for $\\mu$.\n- By CLT, $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n})$ approximately for n large. So, $\\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)$ and a $100(1-\\alpha)\\% $ confidence interval for $\\mu$ is of the form $ Pr(-z_{1-\\frac{\\alpha}{2}} < \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} < z_{1-\\frac{\\alpha}{2}}) = 1-\\alpha$.\n- For known $\\sigma^{2}$, the inequality $ -z_{1-\\frac{\\alpha}{2}} < \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} < z_{1-\\frac{\\alpha}{2}} \\Leftrightarrow \\bar{X}-z_{1-\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar{X}+z_{1-\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}}$ and $(\\hat{\\mu}_{L} = \\bar{X}-z_{1-\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}}, \\hat{\\mu}_{U}= \\bar{X}+z_{1-\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}})$ gives a $100(1-\\alpha)\\%$ confidence interval for $\\mu$.\n- So the $100(1-\\alpha)\\%$ confidence interval is of the form $\\bar{X} \\pm z_{1-\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}}$ where $z_{1-\\frac{\\alpha}{2}}$ can be found by `qnorm(1-alpha/2)`.\n- For unknown $\\sigma^{2}$, we substitute with the sample variance $s^{2}$ and $\\frac{\\bar{X}-\\mu}{\\frac{s}{\\sqrt{n}}} \\sim t_{v}, v = n-1$ .\n- So the $100(1-\\alpha)\\%$ confidence interval is of the form $\\bar{X} \\pm t_{1-\\frac{\\alpha}{2},n-1} \\cdot \\frac{\\sigma}{\\sqrt{n}}$ where $t_{1-\\frac{\\alpha}{2},n-1}$ can be found by `qt(1-alpha/2, df=n-1)`.\n- Alternatively, we may use `t.test(x, conf.level=, alt=)`. Use alt='two.sided' for 2-sided CI, alt='less' for upper bound CI, alt='greater' for lower bound CI.\n- Type I error is $Pr(\\textrm{Reject}\\,H_{0}\\,|\\,H_{0}\\,\\textrm{is true}) = \\alpha$\n- Type II error is $Pr(\\textrm{Do not reject}\\,H_{0}\\,|\\,H_{1}\\,\\textrm{is true})$\n- Power of a test is $1-Pr(\\textrm{Type II error})=Pr({\\textrm{Reject}\\,H_{0}\\,|\\,H_{1}\\,\\textrm{is true}})$"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "#### Parametric Tests Involving Interval/Ratio Observations ####\n\n##### One Sample Z/T test #####\n- Used to test for population mean with known or unknown variance e.g. $H_{0}: \\mu = 2, H_{1}: \\mu \\neq 2$.\n- If **known variance and population is normal** or **if sample size is large**, use z-test.\n- If population is approximately normal and sample size is small, use t-test.\n- Relevant R commands: **t.test(x, mu, conf.level, alt)**. Use alt='two.sided' for 2-sided test, alt='greater' for 1-sided (>) test, alt='less' for 1-sided (<) test.\n\n##### One Sample Proportion test #####\n- Used to test for population proportion e.g. $H_{0}: p = 0.5, H_{1}: p \\neq 0.5$\n- Note that the sample proportion $\\hat{p} = \\frac{\\sum_{i=1}^{n} X_{i}}{n}, X_{i} \\stackrel{iid}{\\sim} Bern(p)$ \n- By CLT, $\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}} \\stackrel{iid}{\\sim} N(0,1)$ approximately.\n- Relevant R commands: **prop.test(x, n, p, conf.level, alt)**\n\n##### Two Independent Samples Z/T test #####\n- Used to test the equality of means of two **normally distributed populations**, e.g. $H_{0}$: $\\mu_{1}-\\mu_{2}=0\\,(\\mu_{1} = \\mu_{2})$, $H_{1}$: $\\mu_1-\\mu_{2} \\neq 0$.\n- If known variance, use z-test. Note that $\\bar{X}_{1}-\\bar{X}_{2} \\sim N(\\mu_{1}-\\mu_{2}, \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}})$. The test statistic, $Z = \\frac{(\\bar{X}_{1}-\\bar{X}_{2})-(\\mu_{1}-\\mu_{2})}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}}} \\sim N(0,1)$\n- In the case of unknown variance, we substitute them with the sample variances $s_{1}^{2}, s_{2}^{2}$ for $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$. In the case where $\\sigma_{1}^{2} = \\sigma_{2}^{2}$, we substitute them with the pooled variance, $s_{p}^{2}$. The test statistic would be t-distributed. **Degree of freedom differs in the two cases!**.\n- To determine whether $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$ or $\\sigma_{1}^{2} = \\sigma_{2}^{2}$, we can conduct an F-test for equality of variances using `var.test(s1, s2)`, where s1 and s2 are the samples of two normally distributed population or `var.test(y~x)`, where y is the dependent variable and x is the factor or group variable.\n- When the two samples are from **approximately normal distributed populations, and the sample sizes are large (â‰¥ 30)** then the CLT can be applied. The test statistic, $Z = \\frac{(\\bar{X}_{1}-\\bar{X}_{2})-(\\mu_{1}-\\mu_{2})}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}}}} \\sim N(0,1) $ approximately. We may also consider the cases $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$ or $\\sigma_{1}^{2} = \\sigma_{2}^{2}$.\n- Relevant R commands: **t.test(s1, s2, mu, conf.level, var.equal, alt)** or **t.test(y~x, mu, conf.level, var.equal, alt)** where y is the dependent variable and x is the factor or group variable. \n\n##### Paired Samples T test for Difference of Means #####\n- Used to test the difference of means of paired samples. For example, the weights of individuals before and after a diet. Note that observations may involve different individuals but matched by some criteria (e.g. age, gender).\n- Suppose we have two populations (before and after) with means $\\mu_{1}$ and $\\mu_{2}$ respectively, and variances, $\\sigma^{2}_{1}$ and $\\sigma^{2}_{2}$ respectively. \n- Denote the paired sample as $\\{(X_{11}, X_{21}), (X_{12},X_{22}), .... (X_{1n},X_{2n})\\}$, where n is the sample size of each population (1 and 2). \n- Let $D_{i} = X_{1i}-X_{2i}$ be the difference of each pair and assume that $D = \\displaystyle \\sum_{i=1}^{n} D_{i} \\sim N(\\mu_{D}, \\sigma^{2}_{D})$. We now apply the **one sample T test** with $H_{0}$: $\\mu_{D} = 0$ against $H_{1}$: $\\mu_{D} \\neq 0$.\n- The test statistic $ \\frac{\\bar{D}-\\mu_{D}}{\\sqrt{\\frac{s_{D}^{2}}{n}}} \\sim t_{v}, v = n-1, s_{D}^{2} = \\frac{1}{n-1} \\displaystyle \\sum_{i=1}^{n} (D_{i}-\\bar{D})^{2} $ If n is large, the test statistic is $\\sim N(0,1)$ approximately (use z-test).\n- **Important Note** Ensure subject id of dataset is sorted (for each group) before carrying out the below test!\n- Relevant R commands: **t.test(d, mu, conf.level, alt)**, where d is the vector of differences $D_{i}$ or **t.test(s1, s2, mu, conf.level, paired=TRUE, alt)**.\n\n##### One-Way ANOVA for Multiple Independent Samples #####\n- Used to test the equality of means of >2 **normally distributed or approximately normal populations**. For example, to compare the mean weights of individuals by smoking status (Smoker, Ex-smoker, Non-smoker).\n- The hypotheses are usually formulated as $H_{0}$: $\\mu_{1} = \\mu_{2} = \\dots = \\mu_{k}, k \\ge 3$, $H_{1}$: $\\mu_{i} \\neq \\mu_{j}$ for some $i$ and $j$, or not all $\\mu_{i}$ are equal. \n- **Key Assumptions**: (1) All sample observations from each population are independent, (2) Population variances are equal.\n- Relevant R commands: **aov(y~x)**, where y is the dependent variable and x is the factor variable. Use summary() to show p-value.\n- When $H_{0}$ is rejected, we may perform pairwise comparisons to determine which samples are different using `pairwise.t.test(y, x, p.adj='none')`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "#### Parametric Tests Involving Categorical Observations ####\n\n##### $\\chi^{2}$ Goodness of Fit test #####\n- Goodness-of-fit tests are used to compare proportions of levels of a categorical variable to theoretical proportions.\n- For example, to test whether the number of boys in a 5-children family follows binomial distribution.\n- The hypotheses are usually formulated as $H_{0}$: no significant difference between observed and expected values of hypothesized distribution, $H_{1}$: there is a significant difference between the observed and the expected values.\n- In order to calculate the expected values, we need to estimate the parameters of the fitted distribution using sample data. \n- In this case, suppose we have a sample of 500 families with 5 children, where $X$ is the rv number of boys in a 5-children family and we want to fit $X \\sim Bin(n,p)$. We estimate $p$ with the sample mean, $\\bar{x} = \\frac{\\textrm{Total no. of boys in sample}}{\\textrm{Total no. of families sampled}}$\n- The chi-squared test statistic, $\\chi^{2} = \\displaystyle \\sum_{x}\\frac{(f_{x}-E_{x})^{2}}{E_{x}}$, where $f_{x}$ and $E_{x}$ are the observed and expected frequencies at $X=x$.\n- $\\chi^{2} \\sim \\chi^{2}_{v}$ where $v = \\textrm{levels of category} - 1 - \\textrm{# of parameters estimated}$.\n- We reject $H_{0}$ if $\\chi^{2} \\ge \\chi^{2}_{v, \\alpha}$. P-value = $Pr(\\chi^{2}_{v} \\ge \\chi^{2})$\n- Relevant R commands: **chisq.test(f, p=)** where f is a vector of observed frequencies and p is an argument of a vector of probabilities corresponding to the levels of a categorical variable. **Note that the degree of freedom cannot be specified here!**.\n\n\n##### Test of Independence between 2 Categorical Variables #####\n- Used to test whether there is a relationship between 2 categorical variables, e.g. $H_{0}$: there is no association between the variables, $H_{1}$: there is an association between the variables.\n- We use the chi-squared test statistic, $\\chi^{2} = \\displaystyle \\sum_{\\textrm{all cells}}\\frac{(\\textrm{observed}-\\textrm{expected})^{2}}{\\textrm{expected}}$ where $\\textrm{expected} = \\frac{\\textrm{row total}  \\times  \\textrm{column total}}{n}$, where n is the total number of observations.\n- We reject $H_{0}$ if $\\chi^{2} \\ge \\chi^{2}_{v, \\alpha}$ where $v = (\\textrm{# of rows}-1) \\times (\\textrm{# of columns}-1)$. P-value = $Pr(\\chi^{2}_{v} \\ge \\chi^{2})$.\n- Relevant R commands: **chisq.test(x)** where x is the contingency table. \n\n\n##### Test of Change in Proportions for Paired Samples #####\n- Used to test whether there is a change in a **binary** response of individuals under different conditions. The same subject may be measured twice (e.g. response of patients before and after a certain treatment) or may be different individuals matched by some criteria (e.g. age, gender).\n- Thus $H_{0}$: number of changes in either level is equally likely, $H_{1}$: number of changes in either level is not equally likely.\n- Relevant R commands: mcnemar.test(t) where t is the contingency table."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "#### Non-parametric Tests ####\n- Non-parametric tests apply to samples where the underlying population does not assume a parametric distribution or model. Useful when samples do not satisfy normality assumptions or when we do not want to make assumptions on the underlying distribution.\n- For samples involving ordinal data (eg. preferences, pain/satisfaction levels), non-parametric tests are usually used.\n\n##### Quantile Test #####\n- Used to test for the $p^{th}$ quantile of a sample $\\{X_{1},...,X_{n}\\}$ i.e. want to find $x_{p}$ s.t $Pr(X\\le x_{p}) = p$. \n- The hypotheses are usually formulated as $H_{0}$: $p^{th}$ quantile is $x_{p}$, $H_{1}$: $p^{th}$ quantile is not $x_{p}$.\n- Relevant R commands: See notes.\n\n##### Wilcoxon Rank Sum (Mann-Whitney) Test #####\n- Used to test whether two independent samples are identically distributed.\n- Assuming X and Y are two continuous distributions, if X and Y are identically distributed then $Pr(X<Y) = Pr(Y<X)$. Note that $Pr(X<Y) + Pr(Y<X) + Pr(Y=X) = 1$ so $Pr(Y=X)=0 \\Rightarrow Pr(X<Y) = Pr(Y<X) = \\frac{1}{2}$.\n- The hypotheses can thus be formulated as $H_{0}$: $Pr(X<Y) = \\frac{1}{2}$, $H_{1}$: $Pr(X<Y) \\neq \\frac{1}{2}$.\n- Intuitively, if X and Y are identically distributed, then each rank $R_{i}$ is equally likely to be assigned to an observation from X or Y.\n- Relevant R commands: **wilcox.text(s1, s2)**\n\n##### Kruskal-Wallis Test #####\n- An extension of the Rank Sum Test, used to test whether >2 independent samples are identically distributed.\n- The hypotheses can thus be formulated as $H_{0}$: all $k$ population distributions are identical, $H_{1}$: not all $k$  population distributions are identical.\n- Relevant R commands: **kruskal.test(y, x)**\n\n##### Sign Test for Paired Samples #####\n- Used to test whether there is a change in observations between paired samples. Suppose we have the paired samples denoted $\\{(X_{1}, Y_{1}), (X_{2},Y_{2}), .... (X_{n},Y_{n})\\}$.\n- Within each pair $(X_{i}, Y_{i})$, a comparison is made and classified as `+` if $X_{i} < Y_{i}$, `-` if$ X_{i} > Y_{i}$, `0` if $X_{i}=Y_{i}$.\n- Want to test: $H_{0}$: $Pr(+)=Pr(-)$, $H_{1}$: $Pr(+) \\neq Pr(-)$ (May be left or right tail test).\n- Under $H_{0}$: the test statistic $T$ = number of `+` $\\sim Bin(n, 0.5)$, assuming no ties **(n is the total outcomes of `+` and `-`)**.\n- The p-value can be calculated using `binom.test(t, n, alternative)`, where t is the test statistic, n is the total outcomes of `+` and `-` and alternative='less' or 'greater' (one-tail), 'two.sided' (two-tail).\n\n##### Wilcoxon Signed Rank Test #####\n- Same purpose as the Sign Test, but takes into account the magnitude of the differences between the samples.\n- Want to test $H_{0}$: Distribution of $X$ and $Y$ are identical, $H_{1}$: Distribution of $X$ is not identical to $Y$. (One-sided $H_{1}$: Values of $X$ tend to be smaller (or bigger) than $Y$).\n- Relevant R commands: **wilcox.test(y, x, paired=TRUE, correct)**\n\n##### Friedman Test for Multiple Dependent Samples #####\n- In some situations, a subject (or matched subjects) may be measured more than twice (under more than 2 different experimental conditions), e.g. to measure efficacy of a drug when a patient is given more than 2 dose levels. \n- Relevant R commands: **friedman.test(y, x, i)**, where y is the dependent variable, x is the factor variable of treatment groups, and i is vector of blocks (subjects)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Tests of Correlation\n\n##### Pearson's Correlation Coefficient\n- Used to measure the strength and direction of a linear relationship is between two (continuous) variables.\n- **Key assumption**: Both variables must be normally (or approximately normal) distributed.\n- Want to test $H_{0}:\\rho = 0$ against $H_{1}:\\rho \\neq 0$. \n- Alternatively, we may test $H_{0}:\\rho \\leq 0$ against $H_{1}:\\rho > 0$ or $H_{0}:\\rho \\ge 0$ against $H_{1}:\\rho < 0$.\n- Relevant R commands: **cor.test(x,y,alt)**, where x is the independent variable and y is the dependent variable."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.5.3",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}